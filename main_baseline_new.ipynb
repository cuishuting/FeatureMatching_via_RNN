{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_baseline_new\n",
    "### The first part: using model: sklearn.linear_model.LogisticRegression\n",
    "a unit test, which pull the first non-missing observation for each person for each temporal feature\n",
    "and concatenate these to the static features and the corresponding mask matrix. \n",
    "The model is sklearn.linear_model.LogisticRegression to check whether the above predictor is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = np.load(\"./MIMIC_timeseries/24hours/series/imputed-normed-ep_1_24.npz\", allow_pickle=True)\n",
    "temporal_feature_dim = 12\n",
    "static_feature_dim = 5\n",
    "targets_dim = 21\n",
    "adm_num = len(org_data['ep_tdata'])\n",
    "temporal_feature = np.zeros((adm_num, temporal_feature_dim))\n",
    "temporal_mask = np.zeros((adm_num, temporal_feature_dim))\n",
    "static_feature = np.zeros((adm_num, static_feature_dim))\n",
    "targets = np.zeros((adm_num, targets_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(adm_num):\n",
    "    for j in range(temporal_feature_dim):\n",
    "        temp_f = org_data[\"ep_tdata\"][i][:, j]\n",
    "        temp_m = org_data[\"ep_tdata_masking\"][i][:, j]\n",
    "        if any(temp_m):\n",
    "            temporal_feature[i, j] = temp_f[temp_m == True][0]\n",
    "            temporal_mask[i, j] = 1\n",
    "        else:\n",
    "            temporal_feature[i, j] = 0\n",
    "            temporal_mask[i, j] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_feature = org_data[\"adm_features_all\"]\n",
    "y_icd9 = org_data[\"y_icd9\"]\n",
    "y_mor = org_data[\"y_mor\"]\n",
    "targets = np.concatenate((y_icd9, y_mor), 1)\n",
    "predictors = np.concatenate((static_feature, temporal_feature, temporal_mask), 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, targets, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shutingcui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/shutingcui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf_models_list = []\n",
    "for i in range(targets_dim):\n",
    "    clf_models_list.append(LogisticRegression(max_iter=1000).fit(X_train, y_train[:, i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "auprc_lists = np.zeros(targets_dim)\n",
    "auroc = np.zeros(targets_dim)\n",
    "\n",
    "for i in range(targets_dim):\n",
    "    cur_y_true = y_test[:, i]\n",
    "    cur_y_score = clf_models_list[i].predict_proba(X_test)[:,1] \n",
    "    # \"y_score\" used to compute auprc/auroc is the probability estimates of the positive class\n",
    "    # through \"clf_models_list[i].classes_\" we know the order for each target class's label is [0 1]\n",
    "    auprc_lists[i] = average_precision_score(cur_y_true, cur_y_score)\n",
    "    auroc[i] = roc_auc_score(cur_y_true, cur_y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44669215, 0.23952274, 0.77001515, 0.47824311, 0.37871609,\n",
       "       0.32696577, 0.94060067, 0.6018682 , 0.59582595, 0.70220011,\n",
       "       0.05419829, 0.14603482, 0.23969321, 0.08384789, 0.41713585,\n",
       "       0.10544544, 0.04146463, 0.50384245, 0.5575825 , 0.36984488,\n",
       "       0.23998469])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auprc_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25238176, 0.17106159, 0.67786662, 0.36415447, 0.31753998,\n",
       "       0.28317455, 0.82332426, 0.48205172, 0.38618578, 0.38618578,\n",
       "       0.00425315, 0.10079959, 0.18747873, 0.0358115 , 0.31413746,\n",
       "       0.08225587, 0.02943178, 0.44683566, 0.46444369, 0.33370194,\n",
       "       0.10037428])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_for_auprc = np.zeros(21)\n",
    "for i in range(targets_dim):\n",
    "    baseline_for_auprc[i] = np.count_nonzero(y_test[:,i]) / len(y_test[:, i])\n",
    "baseline_for_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_model</th>\n",
       "      <td>0.447</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   t1     t2     t3     t4     t5     t6     t7     t8     t9  \\\n",
       "base            0.252  0.171  0.678  0.364  0.318  0.283  0.823  0.482  0.386   \n",
       "logistic_model  0.447  0.240  0.770  0.478  0.379  0.327  0.941  0.602  0.596   \n",
       "\n",
       "                  t10    t11    t12    t13    t14    t15    t16    t17    t18  \\\n",
       "base            0.386  0.004  0.101  0.187  0.036  0.314  0.082  0.029  0.447   \n",
       "logistic_model  0.702  0.054  0.146  0.240  0.084  0.417  0.105  0.041  0.504   \n",
       "\n",
       "                  t19    t20   t21  \n",
       "base            0.464  0.334  0.10  \n",
       "logistic_model  0.558  0.370  0.24  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = []\n",
    "for i in range(1, 22):\n",
    "    col_names.append(\"t\"+str(i))\n",
    "auprc_compares = np.zeros((2, targets_dim))\n",
    "auprc_compares[0] = baseline_for_auprc\n",
    "auprc_compares[1] = auprc_lists\n",
    "auprc = pd.DataFrame(auprc_compares, columns=col_names, index = ['base', \"logistic_model\"])\n",
    "auprc = auprc.round(3)\n",
    "auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t1     t2     t3     t4     t5     t6     t7    t8     t9   t10   t11  \\\n",
       "0  0.694  0.609  0.633  0.635  0.576  0.561  0.806  0.63  0.663  0.76  0.94   \n",
       "\n",
       "     t12    t13    t14   t15   t16    t17    t18    t19    t20    t21  \n",
       "0  0.613  0.586  0.686  0.61  0.57  0.619  0.561  0.613  0.548  0.702  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_df = pd.DataFrame(auroc.reshape(1, targets_dim), columns=col_names)\n",
    "auroc_df = auroc_df.round(3)\n",
    "auroc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second part\n",
    "Predictors are the same with the first part. The change is on the model, using pytorch linear function to see whether has the same result with the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_feature = temporal_feature.astype(np.float32)\n",
    "static_feature = static_feature.astype(np.float32)\n",
    "temporal_mask = temporal_mask.astype(np.float32)\n",
    "targets = targets.astype(np.float32)\n",
    "\n",
    "temporal_feature_tensor = torch.tensor(temporal_feature) # shape: [35623, 12]\n",
    "temporal_mask_tensor = torch.tensor(temporal_mask) # shape: [35623, 12]\n",
    "static_feature_tensor = torch.tensor(static_feature) # shape: [35623, 5]\n",
    "targets_tensor = torch.tensor(targets) # shape: [35623, 21]\n",
    "input_tensor = torch.cat((static_feature_tensor, temporal_feature_tensor, temporal_mask_tensor), 1) # [35623, 29]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForBaselineModel(Dataset):\n",
    "    def __init__(self, input_tensors, target_tensors):\n",
    "        self.inputs = input_tensors\n",
    "        self.targets = target_tensors\n",
    "    def __len__(self):\n",
    "        return(len(self.inputs))\n",
    "    def __getitem__(self, idx):\n",
    "        cur_input = self.inputs[idx]\n",
    "        cur_target = self.targets[idx]\n",
    "        return cur_input, cur_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, temporal_dim, mask_dim, static_dim, targets_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.input_dim = temporal_dim + mask_dim + static_dim\n",
    "        self.targets_dim = targets_dim\n",
    "        self.logistic_layers = nn.ModuleList([nn.Sequential(nn.Linear(self.input_dim, 20), \n",
    "                                                            nn.ReLU(),\n",
    "                                                            nn.Linear(20, 10),\n",
    "                                                            nn.ReLU(),\n",
    "                                                            nn.Linear(10, 1),\n",
    "                                                            nn.Sigmoid()) \n",
    "                                              for t in range(self.targets_dim)])\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        predict_list = torch.zeros(self.targets_dim)\n",
    "        for i in range(self.targets_dim):\n",
    "            predict_list[i] = self.logistic_layers[i](input_data)\n",
    "        return predict_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_baseline = DatasetForBaselineModel(input_tensor, targets_tensor)\n",
    "train_size = int(0.7 * len(dataset_baseline))\n",
    "test_size = len(dataset_baseline) - train_size\n",
    "train_set, test_set = random_split(dataset_baseline, [train_size, test_size], \n",
    "                                   generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-target Learning (21 binary labels)\n",
    "* Currently taking the mean of each target's BCE loss as the training loss for the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \n",
      " 0\n",
      "current time:  10000  current averagetraining loss:  7.075790196865797\n",
      "current time:  20000  current averagetraining loss:  4.305454211711884\n",
      "epoch: \n",
      " 1\n",
      "current time:  10000  current averagetraining loss:  1.8672252172529697\n",
      "current time:  20000  current averagetraining loss:  1.1549715019166469\n",
      "epoch: \n",
      " 2\n",
      "current time:  10000  current averagetraining loss:  0.5583356417417527\n",
      "current time:  20000  current averagetraining loss:  0.548710470020771\n",
      "epoch: \n",
      " 3\n",
      "current time:  10000  current averagetraining loss:  0.5328971315324307\n",
      "current time:  20000  current averagetraining loss:  0.5335102372169495\n",
      "epoch: \n",
      " 4\n",
      "current time:  10000  current averagetraining loss:  0.5255538522601128\n",
      "current time:  20000  current averagetraining loss:  0.523465727865696\n",
      "epoch: \n",
      " 5\n",
      "current time:  10000  current averagetraining loss:  0.5215799546837807\n",
      "current time:  20000  current averagetraining loss:  0.5204867178499699\n",
      "epoch: \n",
      " 6\n",
      "current time:  10000  current averagetraining loss:  0.5199339968562127\n",
      "current time:  20000  current averagetraining loss:  0.5200093936920166\n",
      "epoch: \n",
      " 7\n",
      "current time:  10000  current averagetraining loss:  0.5200055116117\n",
      "current time:  20000  current averagetraining loss:  0.517751397550106\n",
      "epoch: \n",
      " 8\n",
      "current time:  10000  current averagetraining loss:  0.5203825766742229\n",
      "current time:  20000  current averagetraining loss:  0.5182320722043514\n",
      "epoch: \n",
      " 9\n",
      "current time:  10000  current averagetraining loss:  0.5204604976773262\n",
      "current time:  20000  current averagetraining loss:  0.5183047337830067\n",
      "epoch: \n",
      " 10\n",
      "current time:  10000  current averagetraining loss:  0.5197913497388363\n",
      "current time:  20000  current averagetraining loss:  0.5189630424976349\n",
      "epoch: \n",
      " 11\n",
      "current time:  10000  current averagetraining loss:  0.5195355576276779\n",
      "current time:  20000  current averagetraining loss:  0.5193737930953503\n",
      "epoch: \n",
      " 12\n",
      "current time:  10000  current averagetraining loss:  0.5167496131062508\n",
      "current time:  20000  current averagetraining loss:  0.5206480323374272\n",
      "epoch: \n",
      " 13\n",
      "current time:  10000  current averagetraining loss:  0.5181175392866134\n",
      "current time:  20000  current averagetraining loss:  0.5199956101775169\n",
      "epoch: \n",
      " 14\n",
      "current time:  10000  current averagetraining loss:  0.5205879037976265\n",
      "current time:  20000  current averagetraining loss:  0.5202297955453395\n",
      "epoch: \n",
      " 15\n",
      "current time:  10000  current averagetraining loss:  0.5192663344442845\n",
      "current time:  20000  current averagetraining loss:  0.5191283350288868\n",
      "epoch: \n",
      " 16\n",
      "current time:  10000  current averagetraining loss:  0.5198697619736194\n",
      "current time:  20000  current averagetraining loss:  0.5196612685024738\n",
      "epoch: \n",
      " 17\n",
      "current time:  10000  current averagetraining loss:  0.5209356779158115\n",
      "current time:  20000  current averagetraining loss:  0.5194389187395573\n",
      "epoch: \n",
      " 18\n",
      "current time:  10000  current averagetraining loss:  0.519892415612936\n",
      "current time:  20000  current averagetraining loss:  0.5190124843716621\n",
      "epoch: \n",
      " 19\n",
      "current time:  10000  current averagetraining loss:  0.5196169705092907\n",
      "current time:  20000  current averagetraining loss:  0.5198308663666248\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "temporal_dim = 12\n",
    "mask_dim = 12\n",
    "static_dim = 5\n",
    "targets_dim = 21\n",
    "model = LogisticRegression(temporal_dim, mask_dim, static_dim, targets_dim)\n",
    "criterion = BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.05)\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    print(\"epoch: \\n\", epoch)\n",
    "    cum_loss_1000 = 0\n",
    "    for t, train_batch in enumerate(train_loader):\n",
    "        input_data = train_batch[0]\n",
    "        target_data = train_batch[1]  # shape: [10, 21]\n",
    "        predict_list = torch.zeros((batch_size, targets_dim))\n",
    "        for i in range(batch_size):\n",
    "            predict_list[i] = model(input_data[i])\n",
    "\n",
    "        loss_all_targets = torch.zeros(targets_dim)\n",
    "        for i in range(targets_dim):\n",
    "            loss_all_targets[i] = criterion(predict_list[:, i], target_data[:, i])\n",
    "\n",
    "        MTL_loss = torch.mean(loss_all_targets, 0)\n",
    "        cum_loss_1000 += MTL_loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        MTL_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (t+1) % 1000 == 0:\n",
    "            avg_loss = cum_loss_1000 / 1000\n",
    "            print(\"current time: \", (t+1)*batch_size, \" current averagetraining loss: \",  avg_loss)\n",
    "            cum_loss_1000 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = torch.zeros((test_size, targets_dim))\n",
    "y_score = torch.zeros((test_size, targets_dim))\n",
    "for t, test_batch in enumerate(test_loader):\n",
    "    cur_inputs = test_batch[0] # shape:[10, 29]\n",
    "    cur_targets = test_batch[1] # shape: [10, 21]\n",
    "    y_true[t*batch_size:(t+1)*batch_size, :] = cur_targets\n",
    "    cur_predict = torch.zeros((batch_size, targets_dim))\n",
    "    for i in range(batch_size):\n",
    "        cur_predict[i] = model(cur_inputs[i])\n",
    "    y_score[t*batch_size:(t+1)*batch_size, :] = cur_predict\n",
    "    \n",
    "auprc_list_pytorch_base = np.zeros(targets_dim)\n",
    "auroc_list_pytorch_base = np.zeros(targets_dim)\n",
    "for t_id in range(targets_dim):\n",
    "    cur_y_true = y_true[:, t_id].detach().numpy()\n",
    "    cur_y_score = y_score[:, t_id].detach().numpy()\n",
    "    auprc_list_pytorch_base[t_id] = average_precision_score(cur_y_true, cur_y_score)\n",
    "    auroc_list_pytorch_base[t_id] = roc_auc_score(cur_y_true, cur_y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear_model_pytorch</th>\n",
       "      <td>0.249</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         t1     t2     t3     t4     t5     t6     t7     t8  \\\n",
       "base                  0.252  0.171  0.678  0.364  0.318  0.283  0.823  0.482   \n",
       "linear_model_pytorch  0.249  0.153  0.737  0.361  0.369  0.284  0.936  0.480   \n",
       "\n",
       "                         t9    t10    t11    t12    t13    t14    t15    t16  \\\n",
       "base                  0.386  0.386  0.004  0.101  0.187  0.036  0.314  0.082   \n",
       "linear_model_pytorch  0.353  0.391  0.028  0.092  0.157  0.051  0.306  0.088   \n",
       "\n",
       "                        t17    t18    t19    t20    t21  \n",
       "base                  0.029  0.447  0.464  0.334  0.100  \n",
       "linear_model_pytorch  0.025  0.451  0.461  0.417  0.075  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auprc_compares_pytorch = np.zeros((2, targets_dim))\n",
    "auprc_compares_pytorch[0] = baseline_for_auprc\n",
    "auprc_compares_pytorch[1] = auprc_list_pytorch_base\n",
    "auprc_pytorch = pd.DataFrame(auprc_compares_pytorch, columns=col_names, index = ['base', \"linear_model_pytorch\"])\n",
    "auprc_pytorch = auprc_pytorch.round(3)\n",
    "auprc_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.499</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t1     t2     t3     t4     t5     t6     t7     t8     t9    t10  \\\n",
       "0  0.499  0.484  0.622  0.501  0.571  0.497  0.792  0.501  0.476  0.501   \n",
       "\n",
       "     t11    t12   t13    t14   t15    t16   t17    t18    t19    t20    t21  \n",
       "0  0.934  0.474  0.43  0.646  0.49  0.522  0.43  0.501  0.501  0.545  0.371  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_df_pytorch = pd.DataFrame(auroc_list_pytorch_base.reshape(1, targets_dim), columns=col_names)\n",
    "auroc_df_pytorch = auroc_df_pytorch.round(3)\n",
    "auroc_df_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
