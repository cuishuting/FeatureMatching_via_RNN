{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_baseline_new\n",
    "\n",
    "## The first part:\n",
    "* **model**: 21 separate sklearn.linear_model.LogisticRegression models, each for one target\n",
    "* **training data**: \n",
    "    * predictors = np.concatenate((**static_feature, temporal_feature, temporal_mask**), 1), shape: [adm_num, 29]\n",
    "        * **static_feature** : shape: [adm_num, static_feature_dim(5)]\n",
    "        * **temporal_feature** : shape: [adm_num, temporal_feature_dim(12)], \n",
    "      variable \"temporal_feature[i,j]\" denotes admission i's first non-missing measurement for feature j, if admission i doesn't have non-missing measurement, then temporal_feature[i,j] == 0\n",
    "        * **temporal_mask** : shape: [adm_num, temporal_feature_dim(12)], temporal_mask[i,j] == 1 if admission i has non-missing measurement for feature j, otherwise, temporal_mask[i,j] == 0\n",
    "* **labels**:\n",
    "    * targets = np.concatenate((y_icd9, y_mor), 1), shape: [adm_num, 21]\n",
    "        * y_icd9: shape: [adm_num, 20]\n",
    "        * y_mor: shape: [adm_num, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = np.load(\"./MIMIC_timeseries/24hours/series/imputed-normed-ep_1_24.npz\", allow_pickle=True)\n",
    "temporal_feature_dim = 12\n",
    "static_feature_dim = 5\n",
    "targets_dim = 21\n",
    "adm_num = len(org_data['ep_tdata'])\n",
    "temporal_feature = np.zeros((adm_num, temporal_feature_dim))\n",
    "temporal_mask = np.zeros((adm_num, temporal_feature_dim))\n",
    "static_feature = np.zeros((adm_num, static_feature_dim))\n",
    "targets = np.zeros((adm_num, targets_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(adm_num):\n",
    "    for j in range(temporal_feature_dim):\n",
    "        temp_f = org_data[\"ep_tdata\"][i][:, j]\n",
    "        temp_m = org_data[\"ep_tdata_masking\"][i][:, j]\n",
    "        if any(temp_m):\n",
    "            temporal_feature[i, j] = temp_f[temp_m == True][0]\n",
    "            temporal_mask[i, j] = 1\n",
    "        else:\n",
    "            temporal_feature[i, j] = 0\n",
    "            temporal_mask[i, j] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_feature = org_data[\"adm_features_all\"]\n",
    "y_icd9 = org_data[\"y_icd9\"]\n",
    "y_mor = org_data[\"y_mor\"]\n",
    "targets = np.concatenate((y_icd9, y_mor), 1)\n",
    "predictors = np.concatenate((static_feature, temporal_feature, temporal_mask), 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, targets, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shutingcui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/shutingcui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf_models_list = []\n",
    "for i in range(targets_dim):\n",
    "    clf_models_list.append(LogisticRegression(max_iter=1000).fit(X_train, y_train[:, i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "auprc_lists = np.zeros(targets_dim)\n",
    "auroc = np.zeros(targets_dim)\n",
    "\n",
    "for i in range(targets_dim):\n",
    "    cur_y_true = y_test[:, i]\n",
    "    cur_y_score = clf_models_list[i].predict_proba(X_test)[:,1] \n",
    "    # \"y_score\" used to compute auprc/auroc is the probability estimates of the positive class\n",
    "    # through \"clf_models_list[i].classes_\" we know the order of target class is [0 1], so using [:,1] \n",
    "    # will return the probability estimates of the positive class \n",
    "    auprc_lists[i] = average_precision_score(cur_y_true, cur_y_score)\n",
    "    auroc[i] = roc_auc_score(cur_y_true, cur_y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44669215, 0.23952274, 0.77001515, 0.47824311, 0.37871609,\n",
       "       0.32696577, 0.94060067, 0.6018682 , 0.59582595, 0.70220011,\n",
       "       0.05419829, 0.14603482, 0.23969321, 0.08384789, 0.41713585,\n",
       "       0.10544544, 0.04146463, 0.50384245, 0.5575825 , 0.36984488,\n",
       "       0.23998469])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auprc_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline for auprc of each target is computed as the positive rate in each target's class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25238176, 0.17106159, 0.67786662, 0.36415447, 0.31753998,\n",
       "       0.28317455, 0.82332426, 0.48205172, 0.38618578, 0.38618578,\n",
       "       0.00425315, 0.10079959, 0.18747873, 0.0358115 , 0.31413746,\n",
       "       0.08225587, 0.02943178, 0.44683566, 0.46444369, 0.33370194,\n",
       "       0.10037428])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_for_auprc = np.zeros(21)\n",
    "for i in range(targets_dim):\n",
    "    baseline_for_auprc[i] = np.count_nonzero(y_test[:,i]) / len(y_test[:, i])\n",
    "baseline_for_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_model</th>\n",
       "      <td>0.447</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   t1     t2     t3     t4     t5     t6     t7     t8     t9  \\\n",
       "base            0.252  0.171  0.678  0.364  0.318  0.283  0.823  0.482  0.386   \n",
       "logistic_model  0.447  0.240  0.770  0.478  0.379  0.327  0.941  0.602  0.596   \n",
       "\n",
       "                  t10    t11    t12    t13    t14    t15    t16    t17    t18  \\\n",
       "base            0.386  0.004  0.101  0.187  0.036  0.314  0.082  0.029  0.447   \n",
       "logistic_model  0.702  0.054  0.146  0.240  0.084  0.417  0.105  0.041  0.504   \n",
       "\n",
       "                  t19    t20   t21  \n",
       "base            0.464  0.334  0.10  \n",
       "logistic_model  0.558  0.370  0.24  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = []\n",
    "for i in range(1, 22):\n",
    "    col_names.append(\"t\"+str(i))\n",
    "auprc_compares = np.zeros((2, targets_dim))\n",
    "auprc_compares[0] = baseline_for_auprc\n",
    "auprc_compares[1] = auprc_lists\n",
    "auprc = pd.DataFrame(auprc_compares, columns=col_names, index = ['base', \"logistic_model\"])\n",
    "auprc = auprc.round(3)\n",
    "auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t1     t2     t3     t4     t5     t6     t7    t8     t9   t10   t11  \\\n",
       "0  0.694  0.609  0.633  0.635  0.576  0.561  0.806  0.63  0.663  0.76  0.94   \n",
       "\n",
       "     t12    t13    t14   t15   t16    t17    t18    t19    t20    t21  \n",
       "0  0.613  0.586  0.686  0.61  0.57  0.619  0.561  0.613  0.548  0.702  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_df = pd.DataFrame(auroc.reshape(1, targets_dim), columns=col_names)\n",
    "auroc_df = auroc_df.round(3)\n",
    "auroc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second part\n",
    "* **model**: pytorch multi-label binary classification problem \n",
    "    * n_classes: 21 (y_icd9: 20, y_mor: 1)\n",
    "    * n_labels for each class: 2 (0/1)\n",
    "    * model structure: 2-layer MLP, the first layer's size is the same as the input dim; the output layer's size is the same as targets dim(21) without sigmoid activation function here, because we use BCEWithLogitsLoss as loss function, which combines a Sigmoid layer and the BCELoss in one single class\n",
    "* **training data**: \n",
    "    * input_tensor = torch.cat((static_feature_tensor, temporal_feature_tensor, temporal_mask_tensor), 1)\n",
    "    all of the three above variables are the same as those in the first model but the dtype is tensor.\n",
    "    \n",
    "* **labels**:\n",
    "    * targets_tensor = torch.tensor(targets)\n",
    "    same as \"targets\" in the first model but in the tensor form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variables used below:\n",
    "* **temporal_feature**: numpy array with shape: (adm_num, temporal_feature_dim(12)) \n",
    "* **temporal_mask**: numpy array with shape: (adm_num, temporal_feature_dim(12))\n",
    "* **static_feature**: numpy array with shape: (adm_num, static_feature_dim(5))\n",
    "\n",
    "all of the three above variables are the same with those in the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temporal_feature = temporal_feature.astype(np.float32)\n",
    "static_feature = static_feature.astype(np.float32)\n",
    "temporal_mask = temporal_mask.astype(np.float32)\n",
    "targets = targets.astype(np.float32)\n",
    "\n",
    "temporal_feature_tensor = torch.tensor(temporal_feature) # shape: [35623, 12]\n",
    "temporal_mask_tensor = torch.tensor(temporal_mask) # shape: [35623, 12]\n",
    "static_feature_tensor = torch.tensor(static_feature) # shape: [35623, 5]\n",
    "targets_tensor = torch.tensor(targets) # shape: [35623, 21]\n",
    "input_tensor = torch.cat((static_feature_tensor, temporal_feature_tensor, temporal_mask_tensor), 1) # [35623, 29]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForBaselineModel(Dataset):\n",
    "    def __init__(self, input_tensors, target_tensors):\n",
    "        self.inputs = input_tensors\n",
    "        self.targets = target_tensors\n",
    "    def __len__(self):\n",
    "        return(len(self.inputs))\n",
    "    def __getitem__(self, idx):\n",
    "        cur_input = self.inputs[idx]\n",
    "        cur_target = self.targets[idx]\n",
    "        return cur_input, cur_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, temporal_dim, mask_dim, static_dim, targets_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.input_dim = temporal_dim + mask_dim + static_dim\n",
    "        self.targets_dim = targets_dim\n",
    "        self.logistic_layers = nn.Linear(self.input_dim, self.targets_dim)\n",
    "        \n",
    "    def forward(self, input_data, label, pos_weight):\n",
    "        # input_data: [29]\n",
    "        # label: [21]\n",
    "        predict_list = self.logistic_layers(input_data)\n",
    "        criterion = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(predict_list, label)\n",
    "        return {\"predicts\": predict_list, \"loss\": loss}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_baseline = DatasetForBaselineModel(input_tensor, targets_tensor)\n",
    "train_size = int(0.7 * len(dataset_baseline))\n",
    "test_size = len(dataset_baseline) - train_size\n",
    "train_set, test_set = random_split(dataset_baseline, [train_size, test_size], \n",
    "                                   generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### targets_pos_weight\n",
    "Below variable \"targets_pos_weight\" is the parameter \"pos_weight\" of torch.nn.BCEWithLogitsLoss loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9454e+00, 4.8227e+00, 4.6212e-01, 1.7316e+00, 2.1592e+00, 2.4612e+00,\n",
      "        2.0596e-01, 1.0781e+00, 1.5713e+00, 1.5389e+00, 2.3649e+02, 8.8679e+00,\n",
      "        4.3129e+00, 2.6701e+01, 2.1436e+00, 1.0835e+01, 3.2862e+01, 1.2249e+00,\n",
      "        1.1398e+00, 2.0016e+00, 9.1432e+00])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "targets_df = pd.DataFrame(data=targets, columns=col_names)\n",
    "targets_label_distribution = np.zeros((2, targets_dim))\n",
    "for t_id in range(targets_dim):\n",
    "    targets_label_distribution[0][t_id] = targets_df[\"t\" + str(t_id+1)].value_counts()[0]\n",
    "    targets_label_distribution[1][t_id] = targets_df[\"t\" + str(t_id+1)].value_counts()[1]\n",
    "\n",
    "\n",
    "targets_label_distribution_df = pd.DataFrame(data=targets_label_distribution, columns=col_names)\n",
    "targets_pos_weight = torch.zeros(targets_dim)\n",
    "for t_id in range(targets_dim):\n",
    "    targets_pos_weight[t_id] = targets_label_distribution_df[\"t\" + str(t_id+1)][0] / targets_label_distribution_df[\"t\" + str(t_id+1)][1]\n",
    "print(targets_pos_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \n",
      " 0\n",
      "current time:  20000  current averagetraining loss:  6.495218864971399\n",
      "epoch: \n",
      " 1\n",
      "current time:  20000  current averagetraining loss:  0.5843146128237248\n",
      "epoch: \n",
      " 2\n",
      "current time:  20000  current averagetraining loss:  0.5382390275299549\n",
      "epoch: \n",
      " 3\n",
      "current time:  20000  current averagetraining loss:  0.5213832790255547\n",
      "epoch: \n",
      " 4\n",
      "current time:  20000  current averagetraining loss:  0.5229665592849254\n",
      "epoch: \n",
      " 5\n",
      "current time:  20000  current averagetraining loss:  0.5466823604941368\n",
      "epoch: \n",
      " 6\n",
      "current time:  20000  current averagetraining loss:  0.5432641554951668\n",
      "epoch: \n",
      " 7\n",
      "current time:  20000  current averagetraining loss:  0.5541633482813835\n",
      "epoch: \n",
      " 8\n",
      "current time:  20000  current averagetraining loss:  0.5372722413420677\n",
      "epoch: \n",
      " 9\n",
      "current time:  20000  current averagetraining loss:  0.5221352175951004\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "temporal_dim = 12\n",
    "mask_dim = 12\n",
    "static_dim = 5\n",
    "targets_dim = 21\n",
    "model = LogisticRegression(temporal_dim, mask_dim, static_dim, targets_dim)\n",
    "optimizer_second = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.05, amsgrad=True)\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    print(\"epoch: \\n\", epoch)\n",
    "    cum_loss_20000 = 0\n",
    "    for t, train_batch in enumerate(train_loader):\n",
    "        input_data = train_batch[0]\n",
    "        target_data = train_batch[1]  # shape: [10, 21]\n",
    "#         predict_list = torch.zeros((batch_size, targets_dim), requires_grad=True)\n",
    "#         for b_id in range(batch_size):\n",
    "#             predict_list[b_id] = model(input_data[b_id])\n",
    "        result = model(input_data, target_data, targets_pos_weight)\n",
    "#         print(\"target:\\n\")\n",
    "#         print(target_data)\n",
    "#         print(\"predict:\\n\")\n",
    "#         print(predict_list)\n",
    "#         MTL_loss = criterion(predict_list, target_data)\n",
    "        optimizer_second.zero_grad()\n",
    "        result[\"loss\"].backward()\n",
    "        optimizer_second.step()\n",
    "        cum_loss_20000 += result[\"loss\"].item()\n",
    "        if (t+1) % 2000 == 0:\n",
    "            avg_loss = cum_loss_20000 / 20000\n",
    "            print(\"current time: \", (t+1)*batch_size, \" current averagetraining loss: \",  avg_loss)\n",
    "            cum_loss_20000 = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = torch.zeros((test_size, targets_dim))\n",
    "y_score = torch.zeros((test_size, targets_dim))\n",
    "with torch.no_grad():\n",
    "    for t, test_batch in enumerate(test_loader):\n",
    "        cur_inputs = test_batch[0] # shape:[10, 29]\n",
    "        cur_targets = test_batch[1] # shape: [10, 21]\n",
    "        y_true[t*batch_size:(t+1)*batch_size, :] = cur_targets\n",
    "#         cur_predict = torch.zeros((batch_size, targets_dim))\n",
    "#         for i in range(batch_size):\n",
    "#             cur_predict[i] = model(cur_inputs[i])\n",
    "        result = model(cur_inputs, cur_targets, targets_pos_weight)\n",
    "        cur_predict = result[\"predicts\"]\n",
    "        y_score[t*batch_size:(t+1)*batch_size, :] = cur_predict\n",
    "\n",
    "auprc_list_pytorch_base = np.zeros(targets_dim)\n",
    "auroc_list_pytorch_base = np.zeros(targets_dim)\n",
    "for t_id in range(targets_dim):\n",
    "    cur_y_true = y_true[:, t_id].detach().numpy()\n",
    "    cur_y_score = y_score[:, t_id].detach().numpy()\n",
    "    auprc_list_pytorch_base[t_id] = average_precision_score(cur_y_true, cur_y_score)\n",
    "    auroc_list_pytorch_base[t_id] = roc_auc_score(cur_y_true, cur_y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear_model_pytorch</th>\n",
       "      <td>0.408</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         t1     t2     t3     t4     t5     t6     t7     t8  \\\n",
       "base                  0.252  0.171  0.678  0.364  0.318  0.283  0.823  0.482   \n",
       "linear_model_pytorch  0.408  0.216  0.761  0.432  0.371  0.312  0.938  0.589   \n",
       "\n",
       "                         t9    t10    t11    t12    t13    t14    t15    t16  \\\n",
       "base                  0.386  0.386  0.004  0.101  0.187  0.036  0.314  0.082   \n",
       "linear_model_pytorch  0.440  0.696  0.028  0.137  0.234  0.068  0.357  0.107   \n",
       "\n",
       "                        t17    t18    t19    t20    t21  \n",
       "base                  0.029  0.447  0.464  0.334  0.100  \n",
       "linear_model_pytorch  0.043  0.501  0.543  0.353  0.215  "
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auprc_compares_pytorch = np.zeros((2, targets_dim))\n",
    "auprc_compares_pytorch[0] = baseline_for_auprc\n",
    "auprc_compares_pytorch[1] = auprc_list_pytorch_base\n",
    "auprc_pytorch = pd.DataFrame(auprc_compares_pytorch, columns=col_names, index = ['base', \"linear_model_pytorch\"])\n",
    "auprc_pytorch = auprc_pytorch.round(3)\n",
    "auprc_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.645</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t1     t2     t3     t4     t5     t6     t7     t8     t9    t10  \\\n",
       "0  0.645  0.597  0.647  0.566  0.574  0.536  0.795  0.631  0.544  0.735   \n",
       "\n",
       "     t11    t12    t13    t14    t15    t16    t17    t18    t19    t20    t21  \n",
       "0  0.932  0.609  0.576  0.674  0.545  0.568  0.586  0.533  0.606  0.518  0.718  "
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_df_pytorch = pd.DataFrame(auroc_list_pytorch_base.reshape(1, targets_dim), columns=col_names)\n",
    "auroc_df_pytorch = auroc_df_pytorch.round(3)\n",
    "auroc_df_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third part\n",
    "pytorch baseline model with only static features\n",
    "* **model**: the model structure is the same as the second model, but the input size for the first layer changed to static feature dim(5)\n",
    "* **input**: only the static variable\n",
    "* **targets**: still the 21 targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_LR_only_static(Dataset):\n",
    "    def __init__(self, static_data, targets):\n",
    "        self.static_data = static_data\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.static_data)\n",
    "    def __getitem__(self, idx):\n",
    "        cur_static_feature = self.static_data[idx]\n",
    "        cur_targets = self.targets[idx]\n",
    "        return cur_static_feature, cur_targets\n",
    "        \n",
    "static_feature_tensor = torch.tensor(static_feature) # shape: [35623, 5]\n",
    "targets_tensor = torch.tensor(targets) # shape: [35623, 21]\n",
    "dataset_third = Dataset_LR_only_static(static_feature_tensor, targets_tensor)\n",
    "train_set_third, test_set_third = random_split(dataset_third, [train_size, test_size], \n",
    "                                   generator=torch.Generator().manual_seed(42))\n",
    "train_loader_third = DataLoader(train_set_third, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader_third = DataLoader(test_set_third, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_only_static(nn.Module):\n",
    "    def __init__(self, static_dim, targets_dim):\n",
    "        super(LR_only_static, self).__init__()\n",
    "        self.targets_dim = targets_dim\n",
    "        self.static_dim = static_dim\n",
    "        self.LR_layer = nn.Linear(static_dim, self.targets_dim)\n",
    "                                   \n",
    "    def forward(self, input_data, targets, pos_weight):\n",
    "        predict_list = self.LR_layer(input_data)\n",
    "        criterion = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(predict_list, targets)\n",
    "        return {\"predicts\": predict_list, \"loss\": loss}  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \n",
      " 1\n",
      "current time:  20000 current avg loss:  48.75170938867628\n",
      "epoch: \n",
      " 2\n",
      "current time:  20000 current avg loss:  0.5878763862788677\n",
      "epoch: \n",
      " 3\n",
      "current time:  20000 current avg loss:  0.5931688434004784\n",
      "epoch: \n",
      " 4\n",
      "current time:  20000 current avg loss:  0.5738430133223533\n",
      "epoch: \n",
      " 5\n",
      "current time:  20000 current avg loss:  0.5957935884594917\n",
      "epoch: \n",
      " 6\n",
      "current time:  20000 current avg loss:  0.6178610707044602\n",
      "epoch: \n",
      " 7\n",
      "current time:  20000 current avg loss:  0.5869402446866036\n",
      "epoch: \n",
      " 8\n",
      "current time:  20000 current avg loss:  0.5875429338991642\n",
      "epoch: \n",
      " 9\n",
      "current time:  20000 current avg loss:  0.5447198246538639\n",
      "epoch: \n",
      " 10\n",
      "current time:  20000 current avg loss:  0.5845242021918297\n"
     ]
    }
   ],
   "source": [
    "static_dim = 5\n",
    "targets_dim = 21\n",
    "model_third = LR_only_static(static_dim, targets_dim)\n",
    "optimizer_third = optim.Adam(model_third.parameters(), lr=0.001, weight_decay=0.05, amsgrad=True)\n",
    "model_third.train()\n",
    "for epoch in range(10):\n",
    "    print(\"epoch: \\n\", epoch+1)\n",
    "    cum_loss = 0\n",
    "    for t, sample in enumerate(train_loader_third):\n",
    "        input_batch = sample[0] # shape: [10, 5]\n",
    "        target_batch = sample[1] # shape: [10, 21]\n",
    "        optimizer_third.zero_grad()\n",
    "        result = model_third(input_batch, target_batch, targets_pos_weight)\n",
    "        loss = result[\"loss\"]\n",
    "        cum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_third.step()\n",
    "        if (t+1) % 2000 == 0:\n",
    "            print(\"current time: \", ((t+1)*batch_size), \"current avg loss: \", cum_loss / 20000)\n",
    "            cum_loss = 0\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_third.eval()\n",
    "y_true = torch.zeros((test_size, targets_dim))\n",
    "y_score = torch.zeros((test_size, targets_dim))\n",
    "for t, test_batch in enumerate(test_loader_third):\n",
    "    cur_inputs = test_batch[0] # shape:[10, 5]\n",
    "    cur_targets = test_batch[1] # shape: [10, 21]\n",
    "    y_true[t*batch_size:(t+1)*batch_size, :] = cur_targets\n",
    "    cur_predict = model_third(cur_inputs, cur_targets, targets_pos_weight)[\"predicts\"]\n",
    "    y_score[t*batch_size:(t+1)*batch_size, :] = cur_predict\n",
    "    \n",
    "auprc_pytorch_only_static = np.zeros(targets_dim)\n",
    "auroc_pytorch_only_static = np.zeros(targets_dim)\n",
    "for t_id in range(targets_dim):\n",
    "    cur_y_true = y_true[:, t_id].detach().numpy()\n",
    "    cur_y_score = y_score[:, t_id].detach().numpy()\n",
    "    auprc_pytorch_only_static[t_id] = average_precision_score(cur_y_true, cur_y_score)\n",
    "    auroc_pytorch_only_static[t_id] = roc_auc_score(cur_y_true, cur_y_score)\n",
    "\n",
    "# auprc_compares_pytorch = np.zeros((2, targets_dim))\n",
    "# auprc_compares_pytorch[0] = baseline_for_auprc\n",
    "# auprc_compares_pytorch[1] = auprc_list_pytorch_base\n",
    "# auprc_pytorch = pd.DataFrame(auprc_compares_pytorch, columns=col_names, index = ['base', \"linear_model_pytorch\"])\n",
    "# auprc_pytorch = auprc_pytorch.round(3)\n",
    "# auprc_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24844829, 0.16042645, 0.58100458, 0.3391011 , 0.29089721,\n",
       "       0.28303419, 0.93640251, 0.43833232, 0.35229365, 0.50841817,\n",
       "       0.02809732, 0.09159839, 0.22770114, 0.05324051, 0.30570282,\n",
       "       0.08793018, 0.02563605, 0.49508373, 0.53684851, 0.41015563,\n",
       "       0.07498308])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auprc_pytorch_only_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4969658 , 0.50697722, 0.37915694, 0.47107629, 0.43142515,\n",
       "       0.49602762, 0.7922037 , 0.44747195, 0.47420826, 0.63388616,\n",
       "       0.93318912, 0.47184383, 0.57413807, 0.65400816, 0.48943537,\n",
       "       0.52162025, 0.43268394, 0.51960798, 0.60636517, 0.54464194,\n",
       "       0.36763984])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_pytorch_only_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fourth part\n",
    "* **model**: includes a LSTMCell structrue to get embeddings for temporal features, an one-layer MLP to get the prediction for 21 classes\n",
    "* **input**:\n",
    "    * **LSTMCell** : original temporal features on each time stamp's with shape: [batch_size, temporal_feature_dim(12)]\n",
    "    * **one layer MLP**: final hidden state from LSTMCell model concatenate with static features with shape: [batch_size, final_hidden_embedding_dim+static_dim]\n",
    "* **targets**: still the 21 targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables(tensors) than can be used directly\n",
    "# static_feature_tensor    # shape: [35623, 5]\n",
    "# targets_tensor     # shape: [35623, 21]\n",
    "# org_data = np.load(\"./MIMIC_timeseries/24hours/series/imputed-normed-ep_1_24.npz\", allow_pickle=True)\n",
    "\n",
    "class Dataset_fourth(Dataset):\n",
    "    def __init__(self, org_data, static_feature, targets):\n",
    "        self.org_data = org_data\n",
    "        self.static_feature = static_feature # [35623, 5]\n",
    "        self.targets = targets # [35623, 21]\n",
    "        self.temporal_feature = torch.tensor(self.org_data[\"ep_tdata\"].astype(np.float32)) # [35623, 24, 12]\n",
    "        self.temporal_mask = torch.tensor(self.org_data[\"ep_tdata_masking\"].astype(np.float32)) # [35623, 24, 12]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    def __getitem__(self, idx):\n",
    "        cur_temporal_feature = self.temporal_feature[idx] # [24, 12]\n",
    "        cur_static_feature = self.static_feature[idx] # [5]\n",
    "        cur_target = self.targets[idx] # [21]\n",
    "        cur_temporal_mask = self.temporal_mask[idx] # [24, 12]\n",
    "        return cur_temporal_feature, cur_static_feature, cur_target, cur_temporal_mask\n",
    "    \n",
    "dataset_fourth = Dataset_fourth(org_data, static_feature_tensor, targets_tensor)     \n",
    "train_set_fourth, test_set_fourth = random_split(dataset_fourth, [train_size, test_size], \n",
    "                                   generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_set_fourth, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set_fourth, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiModal Class:\n",
    "is used to concatenate final hidden state of the temporal features and the static variables and predict the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModal(nn.Module):\n",
    "    def __init__(self, hidden_dim_lstm, static_feature_dim, targets_dim):\n",
    "        super(MultiModal, self).__init__()\n",
    "        self.hidden_dim_lstm = hidden_dim_lstm\n",
    "        self.static_feature_dim = static_feature_dim\n",
    "        self.targets_dim = targets_dim\n",
    "        self.LR_layer = nn.Linear(self.hidden_dim_lstm+self.static_feature_dim, self.targets_dim)\n",
    "        \n",
    "    def forward(self, final_hidden_embeddings, statics, targets, pos_weight):\n",
    "#         print(\"current final_hidden_embeddings:\\n\")\n",
    "#         print(final_hidden_embeddings.shape) # shape: [batch_size, final_hidden_embeddings_dim]\n",
    "#         print(\"current statics:\\n\")\n",
    "#         print(statics.shape)  # [batch_size, static_size]\n",
    "        input_data = torch.cat((final_hidden_embeddings, statics), 1) # shape: [batch_size, final_hidden_emb_dim+static_dim]\n",
    "        predict_list = self.LR_layer(input_data) \n",
    "        criterion = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(predict_list, targets)\n",
    "        return {\"predicts\": predict_list, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BaseModel_Fourth Class :\n",
    "is used to get final hidden states of the temporal features with batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel_Fourth(nn.Module):\n",
    "    def __init__(self, temporal_feature_dim, hidden_dim_lstm, static_feature_dim, batch_size, seq_len, targets_dim):\n",
    "        super(BaseModel_Fourth, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim_lstm = hidden_dim_lstm\n",
    "        self.temporal_feature_dim = temporal_feature_dim\n",
    "        self.static_feature_dim = static_feature_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.targets_dim = targets_dim\n",
    "        self.initial_hidden_cell =  (torch.randn(self.batch_size, self.hidden_dim_lstm), \n",
    "                                     torch.randn(self.batch_size, self.hidden_dim_lstm))\n",
    "        self.lstm = LSTMCell(self.temporal_feature_dim*2, self.hidden_dim_lstm)\n",
    "        self.mlp = MultiModal(self.hidden_dim_lstm, self.static_feature_dim, self.targets_dim)\n",
    "    def forward(self, temporal_features, temporal_masks, static_features, labels, pos_weight):\n",
    "        input_lstm = torch.cat((temporal_features, temporal_masks), 2) # shape: [batch_size, 24, 12+12]\n",
    "        h, c = self.initial_hidden_cell\n",
    "        for t in range(self.seq_len):\n",
    "            cur_input_lstm = input_lstm[:, t, :] # shape: [batch_size, 12+12]\n",
    "            h, c = self.lstm(cur_input_lstm, (h, c))\n",
    "        result = self.mlp(h, static_features, labels, pos_weight)\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \n",
      " 1\n",
      "current time:  20000 current avg loss:  7.9764210554718975\n",
      "epoch: \n",
      " 2\n",
      "current time:  20000 current avg loss:  0.5924440301656723\n",
      "epoch: \n",
      " 3\n",
      "current time:  20000 current avg loss:  0.6092794606506825\n",
      "epoch: \n",
      " 4\n",
      "current time:  20000 current avg loss:  0.572734712588787\n",
      "epoch: \n",
      " 5\n",
      "current time:  20000 current avg loss:  0.5659458463907242\n",
      "epoch: \n",
      " 6\n",
      "current time:  20000 current avg loss:  0.534124119013548\n",
      "epoch: \n",
      " 7\n",
      "current time:  20000 current avg loss:  0.548151489174366\n",
      "epoch: \n",
      " 8\n",
      "current time:  20000 current avg loss:  0.5710092915773392\n",
      "epoch: \n",
      " 9\n",
      "current time:  20000 current avg loss:  0.5568293450355529\n",
      "epoch: \n",
      " 10\n",
      "current time:  20000 current avg loss:  0.5810946937799454\n"
     ]
    }
   ],
   "source": [
    "temporal_feature_dim = 12\n",
    "hidden_dim_lstm = 30\n",
    "static_feature_dim = 5\n",
    "targets_dim = 21\n",
    "seq_len = 24\n",
    "#  temporal_feature_dim, hidden_dim_lstm, batch_size, seq_len\n",
    "model_fourth = BaseModel_Fourth(temporal_feature_dim, hidden_dim_lstm, static_feature_dim, batch_size, seq_len, targets_dim)\n",
    "optimizer_fourth = optim.Adam(model_fourth.parameters(), lr=0.001, weight_decay=0.05, amsgrad=True)\n",
    "model_fourth.train()\n",
    "for epoch in range(10):\n",
    "    print(\"epoch: \\n\", epoch+1)\n",
    "    cum_loss = 0\n",
    "    for t, item_train in enumerate(train_loader):\n",
    "        cur_targets_batch = item_train[2] # shape: [batch_size, 21]\n",
    "        optimizer_fourth.zero_grad()\n",
    "        item_train[0][item_train[0] != item_train[0]] = 0\n",
    "        cur_temporal_features_batch = item_train[0]\n",
    "        cur_temporal_masks_batch = item_train[3]\n",
    "        cur_static_features_batch = item_train[1]\n",
    "#         temporal_features, temporal_masks, static_features, labels, pos_weight)\n",
    "        cur_result = model_fourth(cur_temporal_features_batch, cur_temporal_masks_batch, cur_static_features_batch, cur_targets_batch, targets_pos_weight) # shape: [batch_size, 21]\n",
    "        loss = cur_result[\"loss\"]\n",
    "        cum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_fourth.step()\n",
    "        if (t + 1) % 2000 == 0:\n",
    "            print(\"current time: \", ((t+1)*batch_size), \"current avg loss: \", cum_loss / 20000)\n",
    "            cum_loss = 0\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fourth.eval()\n",
    "y_true = torch.zeros((test_size, targets_dim))\n",
    "y_score = torch.zeros((test_size, targets_dim))\n",
    "with torch.no_grad():\n",
    "    for t, test_batch in enumerate(test_loader):\n",
    "        test_batch[0][test_batch[0] != test_batch[0]] = 0\n",
    "        cur_targets_batch = test_batch[2]\n",
    "        y_true[t*batch_size: (t+1)*batch_size, :] = cur_targets_batch\n",
    "        cur_temporal_features = test_batch[0]\n",
    "        cur_temporal_masks = test_batch[3]\n",
    "        cur_static_features = test_batch[1]\n",
    "        cur_predicts_batch = model_fourth(cur_temporal_features, cur_temporal_masks, cur_static_features, cur_targets_batch, targets_pos_weight)[\"predicts\"] # \n",
    "#         print(\"predicts size:\\n\")\n",
    "#         print(cur_predicts_batch.shape) # [batch_size, targets_dim]\n",
    "        y_score[t*batch_size: (t+1)*batch_size, :] = cur_predicts_batch\n",
    "    \n",
    "auprc_pytorch_temporal_cat_static = np.zeros(targets_dim)\n",
    "auroc_pytorch_temporal_cat_static = np.zeros(targets_dim)\n",
    "\n",
    "for t_id in range(targets_dim):\n",
    "    cur_y_true = y_true[:, t_id].detach().numpy()\n",
    "    cur_y_score = y_score[:, t_id].detach().numpy()\n",
    "    auprc_pytorch_temporal_cat_static[t_id] = average_precision_score(cur_y_true, cur_y_score)\n",
    "    auroc_pytorch_temporal_cat_static[t_id] = roc_auc_score(cur_y_true, cur_y_score)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24923769, 0.33617761, 0.74845494, 0.40911434, 0.37244693,\n",
       "       0.2961094 , 0.69645145, 0.52730911, 0.4127813 , 0.53441283,\n",
       "       0.02706345, 0.11070036, 0.15767061, 0.0334669 , 0.3282593 ,\n",
       "       0.08124161, 0.04179913, 0.49579534, 0.53556414, 0.41061113,\n",
       "       0.22106324])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auprc_pytorch_temporal_cat_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49894812, 0.69496494, 0.63193605, 0.56214962, 0.57552985,\n",
       "       0.50377338, 0.2120726 , 0.5547802 , 0.53879243, 0.65011933,\n",
       "       0.91204847, 0.52949254, 0.43294003, 0.49843517, 0.51214397,\n",
       "       0.48040042, 0.57202258, 0.5193232 , 0.60636035, 0.54419968,\n",
       "       0.69767675])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_pytorch_temporal_cat_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn_LR_simple_input</th>\n",
       "      <td>0.447</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytorch_LR_simple_input</th>\n",
       "      <td>0.408</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytorch_LR_static_only</th>\n",
       "      <td>0.248</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytorch_LR_temporal_cat_static_org_input</th>\n",
       "      <td>0.249</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             t1     t2     t3     t4     t5  \\\n",
       "baseline                                  0.252  0.171  0.678  0.364  0.318   \n",
       "sklearn_LR_simple_input                   0.447  0.240  0.770  0.478  0.379   \n",
       "pytorch_LR_simple_input                   0.408  0.216  0.761  0.432  0.371   \n",
       "pytorch_LR_static_only                    0.248  0.160  0.581  0.339  0.291   \n",
       "pytorch_LR_temporal_cat_static_org_input  0.249  0.336  0.748  0.409  0.372   \n",
       "\n",
       "                                             t6     t7     t8     t9    t10  \\\n",
       "baseline                                  0.283  0.823  0.482  0.386  0.386   \n",
       "sklearn_LR_simple_input                   0.327  0.941  0.602  0.596  0.702   \n",
       "pytorch_LR_simple_input                   0.312  0.938  0.589  0.440  0.696   \n",
       "pytorch_LR_static_only                    0.283  0.936  0.438  0.352  0.508   \n",
       "pytorch_LR_temporal_cat_static_org_input  0.296  0.696  0.527  0.413  0.534   \n",
       "\n",
       "                                            t11    t12    t13    t14    t15  \\\n",
       "baseline                                  0.004  0.101  0.187  0.036  0.314   \n",
       "sklearn_LR_simple_input                   0.054  0.146  0.240  0.084  0.417   \n",
       "pytorch_LR_simple_input                   0.028  0.137  0.234  0.068  0.357   \n",
       "pytorch_LR_static_only                    0.028  0.092  0.228  0.053  0.306   \n",
       "pytorch_LR_temporal_cat_static_org_input  0.027  0.111  0.158  0.033  0.328   \n",
       "\n",
       "                                            t16    t17    t18    t19    t20  \\\n",
       "baseline                                  0.082  0.029  0.447  0.464  0.334   \n",
       "sklearn_LR_simple_input                   0.105  0.041  0.504  0.558  0.370   \n",
       "pytorch_LR_simple_input                   0.107  0.043  0.501  0.543  0.353   \n",
       "pytorch_LR_static_only                    0.088  0.026  0.495  0.537  0.410   \n",
       "pytorch_LR_temporal_cat_static_org_input  0.081  0.042  0.496  0.536  0.411   \n",
       "\n",
       "                                            t21  \n",
       "baseline                                  0.100  \n",
       "sklearn_LR_simple_input                   0.240  \n",
       "pytorch_LR_simple_input                   0.215  \n",
       "pytorch_LR_static_only                    0.075  \n",
       "pytorch_LR_temporal_cat_static_org_input  0.221  "
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_auprc_compare = np.zeros((5, targets_dim))\n",
    "final_auprc_compare[0] = baseline_for_auprc\n",
    "final_auprc_compare[1] = auprc_lists \n",
    "final_auprc_compare[2] = auprc_list_pytorch_base\n",
    "final_auprc_compare[3] = auprc_pytorch_only_static\n",
    "final_auprc_compare[4] = auprc_pytorch_temporal_cat_static\n",
    "auprc_sum = pd.DataFrame(final_auprc_compare, columns=col_names, \n",
    "                         index = ['baseline', 'sklearn_LR_simple_input', 'pytorch_LR_simple_input', 'pytorch_LR_static_only', 'pytorch_LR_temporal_cat_static_org_input'])\n",
    "auprc_sum = auprc_sum.round(3)\n",
    "auprc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>t11</th>\n",
       "      <th>t12</th>\n",
       "      <th>t13</th>\n",
       "      <th>t14</th>\n",
       "      <th>t15</th>\n",
       "      <th>t16</th>\n",
       "      <th>t17</th>\n",
       "      <th>t18</th>\n",
       "      <th>t19</th>\n",
       "      <th>t20</th>\n",
       "      <th>t21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sklearn_LR</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytorch_LR</th>\n",
       "      <td>0.645</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytorch_LR_static_only</th>\n",
       "      <td>0.497</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pytorch_LR_temporal_cat_static</th>\n",
       "      <td>0.499</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   t1     t2     t3     t4     t5     t6  \\\n",
       "sklearn_LR                      0.694  0.609  0.633  0.635  0.576  0.561   \n",
       "pytorch_LR                      0.645  0.597  0.647  0.566  0.574  0.536   \n",
       "pytorch_LR_static_only          0.497  0.507  0.379  0.471  0.431  0.496   \n",
       "pytorch_LR_temporal_cat_static  0.499  0.695  0.632  0.562  0.576  0.504   \n",
       "\n",
       "                                   t7     t8     t9    t10    t11    t12  \\\n",
       "sklearn_LR                      0.806  0.630  0.663  0.760  0.940  0.613   \n",
       "pytorch_LR                      0.795  0.631  0.544  0.735  0.932  0.609   \n",
       "pytorch_LR_static_only          0.792  0.447  0.474  0.634  0.933  0.472   \n",
       "pytorch_LR_temporal_cat_static  0.212  0.555  0.539  0.650  0.912  0.529   \n",
       "\n",
       "                                  t13    t14    t15    t16    t17    t18  \\\n",
       "sklearn_LR                      0.586  0.686  0.610  0.570  0.619  0.561   \n",
       "pytorch_LR                      0.576  0.674  0.545  0.568  0.586  0.533   \n",
       "pytorch_LR_static_only          0.574  0.654  0.489  0.522  0.433  0.520   \n",
       "pytorch_LR_temporal_cat_static  0.433  0.498  0.512  0.480  0.572  0.519   \n",
       "\n",
       "                                  t19    t20    t21  \n",
       "sklearn_LR                      0.613  0.548  0.702  \n",
       "pytorch_LR                      0.606  0.518  0.718  \n",
       "pytorch_LR_static_only          0.606  0.545  0.368  \n",
       "pytorch_LR_temporal_cat_static  0.606  0.544  0.698  "
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_auroc_compare = np.zeros((4, targets_dim))\n",
    "final_auroc_compare[0] = auroc\n",
    "final_auroc_compare[1] = auroc_list_pytorch_base\n",
    "final_auroc_compare[2] = auroc_pytorch_only_static \n",
    "final_auroc_compare[3] = auroc_pytorch_temporal_cat_static\n",
    "auroc_sum = pd.DataFrame(final_auroc_compare, columns=col_names,\n",
    "                        index = ['sklearn_LR', 'pytorch_LR', 'pytorch_LR_static_only', 'pytorch_LR_temporal_cat_static'])\n",
    "auroc_sum = auroc_sum.round(3)\n",
    "auroc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
